+++
date = '2025-07-07T21:44:00-04:00'
draft = false
title = 'ZQA Devlog 02'
+++

This is part 2 of a series of posts logging development of a RAG system for Zotero that I'm writing in Rust. [Here](https://blog.ryedida.me/posts/devlog/zqa-01/) is Part 1.

# Towards parsing math

The next challenge to tackle is math: how does it work? Or more specifically: how are equations and inline mathematical symbols represented in the PDF content stream, and how can we use that information to parse them? For this, the best way to start is to simply create a very simple PDF with some commonly-used symbols and see what the content stream looks like. It's straightforward to generate this using LaTeX:

```tex
begin

\sum a \sum\limits_{i=0}^n b \int c \int_0^\infty d \int\limits_0^\infty e\mathbb{E}[x]

end
```

Note that we added two delimiter regular text phrases to help us know what parts of the PDF content stream to focus on. It does not matter that the equation makes no sense: we're interested in the content stream generated by this. For testing, it's easiest to simply put this in a test module and simply run that one test, changing the filename as necessary:

```rs
#[cfg(test)]
mod tests {
    /// Get the raw content stream for page `page_num` for the PDF.
    fn get_raw_content_stream(doc: &Document, page_num: usize) -> Result<String, PdfError> {
        let page_id: (u32, u16) = doc
            .page_iter()
            .nth(page_num)
            .ok_or(PdfError::ContentError)?;

        let page_content = doc
            .get_page_content(page_id)
            .map_err(|_| PdfError::ContentError)?;
        let content = String::from_utf8_lossy(&page_content);

        Ok(content.to_string())
    }

    #[test]
    fn test_pdf_content() {
        if env::var("CI").is_ok() {
            // Skip this test in CI environments
            return;
        }

        let path = PathBuf::from("assets").join("symbols.pdf");

        let doc = Document::load(path).unwrap();
        let content = get_raw_content_stream(&doc, 0).unwrap();

        dbg!(content);
    }
}
```

The relevant part of the content stream is here (I've formatted and truncated it so it's clearer):

```
/F28 
9.9626 Tf 
0 -21.821 Td 
[(b)-28(egin)]TJ
/F21 
9.9626 Tf 
110.055 -23.039 Td 
[(X)]TJ
/F31 
9.9626 Tf 
16.051 -9.465 Td 
[(a)]TJ
/F30 
6.9738 Tf 
11.66 12.454 Td 
[(n)]TJ
/F21 
9.9626 Tf 
-4.733 -2.989 Td 
[(X)]TJ
/F30 
6.9738 Tf 
0.742 -21.219 Td 
[(i)]TJ
/F27 
6.9738 Tf 
2.819 0 Td 
[(=0)]TJ
...
...
-215.945 -29.623 Td 
[(end)]TJ 
154.421 -346.835 Td 
[(1)]TJ
ET
```

And if you're wondering, yes: PDFs *are* that chaotic. There's method to the madness, though. All the `/Fxx` commands you see are a change of font. PDF content streams contain a sequence of commands, with the parameters to the commands preceding them--and before you ask, I have absolutely no idea why. So `9.9626 Tf` tells the PDF renderer to set the text font to 9.9626pt (why not exactly 10? This has to do with font rendering and topics that quite frankly, I don't really understand yet). The `Td` commands tell the PDF renderer to move by a specific vertical and horizontal distance, and the `TJ` commands are text arrays, with specific spacing between grapheme clusters. The key here is that these are not *words*, just *grapheme clusters*. This means that kerning considerations during font rendering might mean words get split up (and quite often, they do--another annoying part of PDF parsing). 

But this content stream gives us a hint as to how symbols might be rendered: the super- and subscripts can simply be handled using `Td` commands, but the symbols themselves are achieved through a change of *font*. For example, the first symbol we have is a summation symbol. We see that right after our "begin" delimiter, `pdflatex` changes the font to `F21`. In PDFs, fonts are...a complicated structure. Each page has a resources dictionary, that includes font definitions. The font definitions themselves are dictionaries that map font properties such as the name, its character map, etc. to potentially more dictionaries. In practice, these dictionaries are present elsewhere in the page resources dictionary and the values are simply resource identifiers, but if you put them all together, a good (if slightly inaccurate) mental model is of a triply-nested dictionary. `lopdf` makes parsing the dictionaries themselves pretty simple, so we just need to use the functions it provides:

```rs
    #[test]
    fn test_font_properties() {
        if env::var("CI").is_ok() {
            // Skip this test in CI environments
            return;
        }

        let font_key = "F21";
        let path = PathBuf::from("assets").join("symbols.pdf");

        let doc = Document::load(path).unwrap();
        let page_id = doc.page_iter().next().unwrap();

        // Get the font dictionary for the page
        let fonts = doc.get_page_fonts(page_id).unwrap();

        let font_obj = fonts.get(font_key.as_bytes()).unwrap();
        let font_hash = font_obj.as_hashmap();

        let readable_font_obj: HashMap<String, &Object> = font_hash
            .iter()
            .map(|(k, v)| (String::from_utf8(k.clone()).unwrap(), v))
            .collect();
        dbg!(&readable_font_obj);
    }
```

One other thing to note is that in PDFs, resources, including fonts, are represented by two numbers. Keep this in mind as we read the output of this function.

```
[pdftools/src/parse.rs:673:9] &readable_font_obj = {
    "ToUnicode": 20 0 R,
    "BaseFont": /ETPJKV+CMEX10,
    "Type": /Font,
    "Widths": 47 0 R,
    "LastChar": 90,
    "Subtype": /Type1,
    "FontDescriptor": 53 0 R,
    "FirstChar": 88,
}
```

The first key is the `ToUnicode` character map, which is a pointer to a resource whose ID is the number pair 20, 0. We are not interested in that. We instead care about the font name itself, actually. The part before the + is just a string of random characters: it indicates that only the glyphs used in the document have been embedded into this PDF document, and not the entire font. We care about the rest of the name, `CMEX10`. This is LaTeX's *Computer Modern Math Extension* font. The 10 is the design size of the font. The next piece of the puzzle is to look at the rest of the content stream. The summation symbol has been represented by a single character, capital X. This is our hint to look at the actual font itself. On my system, this is located at `/usr/share/texmf-dist/fonts/afm/public/amsfonts/cm/cmex10.afm`. This path might as well be magic: I simply navigated to `/usr/share/texmf-dist/` (a directory I discovered exists by aggressively hitting tab on every `/usr/` subdirectory), and then simply running `fd cmex`. Then, I looked through the files one by one to see which made any sense. We know we're on the right track, because there's this line in this file:

```
C 88 ; WX 1444 ; N summationdisplay ; B 56 -1400 1387 0 ;
```

Capital X is ASCII 88, so it's promising that this lines up well. It turns out that this method of spelunking through the tex files to find the font mappings works quite well. Sadly, this doesn't use the exact LaTeX commands we used, but it's close enough that we can do some more manual labor, and from here, implementing the text substitutions themselves is trivial:

```rs
// Just add a bunch of these.
pub fn from_cmex(ch: &u8) -> String {
    match ch {
        88 => "\\sum".to_string(), // X
        90 => "\\int".to_string(), // Z
        _ => char::from(*ch).to_string(),
    }
}

/// A type to convert from bytes in math fonts to LaTeX code
type ByteTransformFn = fn(&u8) -> String;

fn font_transform(input: String, transform: ByteTransformFn) -> String {
    input.as_bytes().iter().map(transform).collect::<String>()
}

/// A lazy-loaded hashmap storing conversions from math fonts to LaTeX code
/// Handles most common math fonts, but does not yet support specialized math fonts.
static FONT_TRANSFORMS: Lazy<HashMap<&'static str, ByteTransformFn>> = Lazy::new(|| {
    let mut m: HashMap<&'static str, ByteTransformFn> = HashMap::new();

    m.insert("CMMI5", from_cmmi);
    m.insert("CMMI6", from_cmmi);
    m.insert("CMMI7", from_cmmi);
    m.insert("CMMI8", from_cmmi);
    m.insert("CMMI9", from_cmmi);
    m.insert("CMMI10", from_cmmi);
    m.insert("CMMI12", from_cmmi);

    m.insert("CMSY5", from_cmsy);
    m.insert("CMSY6", from_cmsy);
    m.insert("CMSY7", from_cmsy);
    m.insert("CMSY8", from_cmsy);
    m.insert("CMSY9", from_cmsy);
    m.insert("CMSY10", from_cmsy);

    m.insert("CMEX10", from_cmex);

    m.insert("MSBM5", from_msbm);
    m.insert("MSBM6", from_msbm);
    m.insert("MSBM7", from_msbm);
    m.insert("MSBM8", from_msbm);
    m.insert("MSBM9", from_msbm);
    m.insert("MSBM10", from_msbm);

    m
});
```

This is, admittedly, a bit of a hacky and harcoded solution, but it works, and is easy to re-reverse engineer. In our actual parser, this ends up being a matter of tracking `/F` commands to store state, and then using this hashmap as needed:

```rs
if let Some(transform) = FONT_TRANSFORMS.get(self.cur_font.as_str()) {
    parsed += &font_transform(cur_content[idx1 + 1..idx2].to_string(), *transform);
} else {
    parsed += &cur_content[idx1 + 1..idx2];
}
```

That's all that's needed! We can now test that this works, this time with a *real* test:

```rs
#[test]
fn test_math_parsing_works() {
    let path = PathBuf::from("assets").join("symbols.pdf");

    let content = extract_text(path.to_str().unwrap());
    assert!(content.is_ok());

    let content = content.unwrap();
    for op in [r"\int", r"\sum", r"\infty"] {
        assert!(content.contains(op));
    }
}
```

# Fin

This whole adventure of figuring out how equations are rendered, finding the font file locations, and even figuring out what `CMEX` meant took me a few months, but it's quite nice to see it finally work the way I want--rewriting the content stream into a LaTeX representation, which is what we want stored in our vector database. With this, our PDF parser is in a slightly nicer state, but there's still the issue of tables, images, and more complicated math equations.
